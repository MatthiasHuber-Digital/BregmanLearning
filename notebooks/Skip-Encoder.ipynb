{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-sampling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Various torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ------------------------\n",
    "# get up one directory \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# ------------------------\n",
    "\n",
    "# custom packages\n",
    "import models.aux_funs as maf\n",
    "import optimizers as op\n",
    "import regularizers as reg\n",
    "import train\n",
    "import math\n",
    "import utils.configuration as cf\n",
    "import utils.datasets as ud\n",
    "from utils.datasets import get_data_set, GaussianSmoothing\n",
    "from models.fully_connected import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Fix random seed\n",
    "# -----------------------------------------------------------------------------------\n",
    "random_seed = 3\n",
    "cf.seed_torch(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-syndrome",
   "metadata": {},
   "source": [
    "# Test cases\n",
    "## Test case 0: No Skips, Denoising\n",
    "* No skips (regularization parameter for skips is set to a large value)\n",
    "* Denoising task, no blur is added to the images\n",
    "\n",
    "## Test case 1: No Skips, Deblurring\n",
    "* No skips (regularization parameter for skips is set to a large value)\n",
    "* Deblurring task, blur is added to the images\n",
    "\n",
    "## Test case 2: Skips, Denoising\n",
    "* Skips are allowed but regularized\n",
    "* Denoising task, no blur is added to the images\n",
    "\n",
    "## Test case 3: Skips, Deblurring\n",
    "* Skips are allowed but regularized\n",
    "* Deblurring task, blur is added to the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = 0\n",
    "\n",
    "if test_case == 0:\n",
    "    lamda_0 = 0.07\n",
    "    lamda_1 = 1e10\n",
    "    add_blur = False\n",
    "elif test_case == 1:\n",
    "    lamda_0 = 0.07\n",
    "    lamda_1 = 1e10\n",
    "    add_blur = True\n",
    "elif test_case == 2:\n",
    "    lamda_0 = 0.07\n",
    "    lamda_1 = 28*lamda_0\n",
    "    add_blur = False\n",
    "elif test_case == 3:\n",
    "    lamda_0 = 0.07\n",
    "    lamda_1 = 28*lamda_0\n",
    "    add_blur = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-money",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-advertising",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reshaped_mse_loss(x,y):\n",
    "    return torch.nn.MSELoss()(x,y.view(-1,28*28))\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Parameters\n",
    "# -----------------------------------------------------------------------------------\n",
    "conf_args = {#\n",
    "    # data specification\n",
    "    'data_file':\"../../Data\", 'train_split' : 0.95, 'data_set':\"Encoder-MNIST\",\n",
    "    'add_noise':True, 'noise_std': 0.05, 'add_blur':add_blur,\n",
    "    # cuda\n",
    "    'use_cuda': True, 'num_workers':4, 'cuda_device':1, 'pin_memory':False,\n",
    "    #\n",
    "    'epochs':100, 'loss':reshaped_mse_loss,\n",
    "    # optimizer\n",
    "    'delta': 1.0, 'lr': 0.001, 'lamda_0': lamda_0, 'lamda_1':lamda_1, 'optim':\"AdaBreg\", 'row_group':True,\n",
    "    'reg':reg.reg_l1_l2, 'beta':0.0,\n",
    "    # model\n",
    "    'model_size':7*[28*28], 'act_fun':torch.nn.ReLU(),\n",
    "    # initialization\n",
    "    'sparse_init':0.01, 'r':[1,5,1],\n",
    "    # misc\n",
    "    'random_seed':random_seed, 'eval_acc':False, 'name':'main-Encoder-MNIST', 'super_type':'Encoder'\n",
    "}\n",
    "conf = cf.Conf(**conf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-visiting",
   "metadata": {},
   "source": [
    "# General ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_neurons, out_neurons, idx, act_fun, act_fun_outer):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.act_fun = act_fun\n",
    "        self.act_fun_outer = act_fun_outer\n",
    "        self.linear = nn.Linear(in_neurons, out_neurons)\n",
    "\n",
    "        s = torch.zeros((idx+1,))              \n",
    "                        \n",
    "        self.skips = nn.Parameter(s, requires_grad=True)\n",
    "        self.idx = idx  \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x2 = torch.sum(self.skips.view(1,1,-1) * x,2)\n",
    "        x3 = x[:,:,-1]\n",
    "        \n",
    "        out = self.act_fun_outer(self.act_fun(self.linear(x3)) + x2)\n",
    "        #\n",
    "        return torch.cat((x,out.unsqueeze(2)),2)\n",
    "    \n",
    "class OutBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutBlock, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x[:,:,-1]\n",
    "\n",
    "class fully_skip_connected(nn.Module):\n",
    "    def __init__(self, sizes, act_fn, outer_act_fn = nn.Identity(), mean=0.0, std=1.0):\n",
    "        super(fully_skip_connected, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.num_l = len(sizes)\n",
    "        \n",
    "        self.act_fn = act_fn\n",
    "        layer_list = []\n",
    "        for i in range(self.num_l-1):\n",
    "            layer_list.append(BasicBlock(sizes[i], sizes[i+1], i, self.act_fn, outer_act_fn))\n",
    "            \n",
    "        layer_list.append(OutBlock())\n",
    "            \n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (x-self.mean)/self.std\n",
    "        x = nn.Flatten()(x)\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-generation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# define the model and an instance of the best model class\n",
    "# -----------------------------------------------------------------------------------\n",
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "\n",
    "model_conf = \"fsc\"\n",
    "if model_conf == \"fsc\":\n",
    "    model = fully_skip_connected(conf.model_size, conf.act_fun, **model_kwargs)\n",
    "    best_model = train.best_model(fully_skip_connected(conf.model_size, conf.act_fun, **model_kwargs).to(conf.device))\n",
    "elif model_conf == \"fc\":\n",
    "    model = fully_connected(conf.model_size, conf.act_fun, **model_kwargs)\n",
    "    best_model = train.best_model(fully_connected(conf.model_size, conf.act_fun, **model_kwargs).to(conf.device))\n",
    "    \n",
    "# sparsify\n",
    "maf.sparse_bias_uniform_(model, 0,conf.r[0])\n",
    "maf.sparse_weight_normal_(model, conf.r[1])\n",
    "maf.sparsify_(model, conf.sparse_init, row_group = conf.row_group)\n",
    "model = model.to(conf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# define the model and an instance of the best model class\n",
    "# -----------------------------------------------------------------------------------\n",
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "\n",
    "def init_weights(conf, model):\n",
    "    # sparsify\n",
    "    maf.sparse_bias_uniform_(model, 0, conf.r[0])\n",
    "    maf.sparse_weight_normal_(model, conf.r[1])\n",
    "    maf.sparsify_(model, conf.sparse_init, row_group = conf.row_group)\n",
    "    model = model.to(conf.device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-johnson",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Optimizer\n",
    "# -----------------------------------------------------------------------------------\n",
    "def get_skips(model):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m,'skips'):\n",
    "            yield m.skips\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "def print_skips(model):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m,'skips'):\n",
    "            print((0.001*torch.round(1000*m.skips.data).cpu()))\n",
    "            \n",
    "def skips_to_list(model):\n",
    "    skips = []\n",
    "    for m in model.modules():\n",
    "        if hasattr(m,'skips'):\n",
    "            skips.append(m.skips.data.tolist())\n",
    "    return skips\n",
    "    \n",
    "\n",
    "def init_opt(conf, model):\n",
    "    # Get access to different model parameters\n",
    "    weights_linear = maf.get_weights_linear(model)\n",
    "    biases = maf.get_bias(model)\n",
    "    skips = get_skips(model)\n",
    "\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # Initialize optimizer\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    reg_0 = conf.reg(lamda=conf.lamda_0)\n",
    "    reg_1 = reg.reg_l1(lamda=conf.lamda_1)\n",
    "\n",
    "    if conf.optim == \"SGD\":\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=conf.lr, momentum=conf.beta)\n",
    "    elif conf.optim == \"AdaBreg\":\n",
    "        opt = op.AdaBreg([{'params': weights_linear, 'lr' : conf.lr, 'reg' : reg_0},\n",
    "                           {'params': biases, 'lr': conf.lr},\n",
    "                           {'params': skips, 'lr':conf.lr, 'reg':reg_1}])\n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5,threshold=0.01)\n",
    "    \n",
    "    return opt, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-passage",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_params = False\n",
    "if save_params:\n",
    "    conf.write_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Prepare Data\n",
    "# -----------------------------------------------------------------------------------\n",
    "smoothing = GaussianSmoothing(1, 9, 5.0)   \n",
    "transform = []\n",
    "\n",
    "if conf.add_blur:\n",
    "    transform.append(smoothing)\n",
    "\n",
    "if conf.add_noise:\n",
    "    transform.append(ud.add_noise(std=conf.noise_std,device='cpu'))\n",
    "    \n",
    "transform = transforms.Compose(transform)   \n",
    "\n",
    "train_set,test_set = ud.get_mnist(conf)\n",
    "train_set = ud.get_augmented_dataset(conf, train_set,transform)\n",
    "test_set = ud.get_augmented_dataset(conf, test_set,transform)\n",
    "train_loader, valid_loader, test_loader = ud.train_valid_test_split(conf, train_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-savannah",
   "metadata": {},
   "source": [
    "# Initialize history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked = ['loss', 'node_sparse']\n",
    "train_hist = {}\n",
    "val_hist = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-mumbai",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-timeline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Reinit weigts and the corresponding optimizer\n",
    "# -----------------------------------------------------------------------------------\n",
    "model = init_weights(conf, model)\n",
    "opt, scheduler = init_opt(conf, model)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# train the model\n",
    "# -----------------------------------------------------------------------------------\n",
    "for epoch in range(conf.epochs):\n",
    "    print(25*\"<>\")\n",
    "    print(50*\"|\")\n",
    "    print(25*\"<>\")\n",
    "    print('Epoch:', epoch)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # train step, log the accuracy and loss\n",
    "    # ------------------------------------------------------------------------\n",
    "    train_data = train.train_step(conf, model, opt, train_loader)\n",
    "\n",
    "    # update history\n",
    "    for key in tracked:\n",
    "        if key in train_data:\n",
    "            var_list = train_hist.setdefault(key, [])\n",
    "            var_list.append(train_data[key])        \n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # validation step\n",
    "    val_data = train.validation_step(conf, model, opt, valid_loader)\n",
    "\n",
    "    print_skips(model)\n",
    "    # update history\n",
    "    for key in tracked:\n",
    "        if key in val_data:\n",
    "            var = val_data[key]\n",
    "            if isinstance(var, list):\n",
    "                for i, var_loc in enumerate(var):\n",
    "                    key_loc = key+\"_\" + str(i)\n",
    "                    var_list = val_hist.setdefault(key_loc, [])\n",
    "                    val_hist[key_loc].append(var_loc)\n",
    "            else:\n",
    "                var_list = val_hist.setdefault(key, [])\n",
    "                var_list.append(var)\n",
    "\n",
    "\n",
    "    scheduler.step(train_data['loss'])\n",
    "    print(\"Learning rate:\",opt.param_groups[0]['lr'])\n",
    "    best_model(train_data['acc'], val_data['acc'], model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
